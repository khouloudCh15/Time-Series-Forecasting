{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explore the Data with *pandas*\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electronic Health Records (EHRs) contain a wealth of patient medical information that can:\n",
    "* Save valuable time when an emergency arises\n",
    "* Eliminate unnecesary treatment and tests\n",
    "* Prevent potentially life-threatening mistakes\n",
    "* Improve the overall quality of care a patient receives when seeking medical assistance\n",
    "\n",
    "Children's Hospital Los Angeles (CHLA) wanted to know if the records could be mined to yield early warning signs of patients that may require extra care or an indication of the severity of a patient's illness.  In this lab we have access to the work and results of CHLA's applied use of deep neural networks on EHRs belonging to roughly 5,000 pediatric ICU patients.\n",
    "\n",
    "We will start by exploring the data using the python library [*pandas*](http://pandas.pydata.org) to manage the dataset provided in [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 [**Set Up the Environment**](#01_setup)<br>\n",
    "1.2 [**Load the Data with *pandas***](#01_pandas)<br>\n",
    "1.3 [**Visualize the Data**](#01_explore)<br>\n",
    "&nbsp; &nbsp; &nbsp;1.3.1 [Example: View Data from a Single Encounter](#01_single)<br>\n",
    "&nbsp; &nbsp; &nbsp;1.3.2 [Exercise: Observation Count Histogram](#01_ex_nobs)<br>\n",
    "&nbsp; &nbsp; &nbsp;1.3.3 [Exercise: Length of PICU Stay Histogram](#01_ex_time)<br>\n",
    "1.4 [**Save a *pandas* DataFrame**](#01_save)<br>\n",
    "[**Solutions**](#01_solutions)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01_setup\"></a>\n",
    "## 1.1 Set Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the libraries we need into our Python workspace.  We need *os* for access to the file system,  *NumPy* for fast array math, *pandas* for data management, and  *MatPlotLib* for visualization.<br>\n",
    "Execute the cell below to import these libraries and define the location of the data in the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd              \n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "\n",
    "# Configure the notebook to display plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_dir = '/dli/task/data/hx_series'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify the file paths in the data folder which contains training and validation datasets stored in HDF5 format: inputs (X) and their associated labels (y).\n",
    "[HDF5](http://www.hdfgroup.org/) stands for \"hierarchical data format version number 5\".  The HDF format is designed specifically to store and organize large amounts of scientific data and was originally designed by the [National Center for Supercomputing Applications](https://en.wikipedia.org/wiki/National_Center_for_Supercomputing_Applications).  Common file extensions include `.hdf`, `.hdf5`, or simply `.h5`.  The HDF format has become very popular and is well maintained.  As a result, HDF5 is a flexible and robust format having API support in most languages and library compatibilty with Windows, OS X and Linux. It is important to note that HDF is a binary format and hence lacks the human readable transparency of text based CSV files.  However, HDF file format has faster performance and more efficient storage size.  It also scales well from small proof-of-concept ideas to [very large operational projects](https://www.hdfgroup.org/eos_vignette/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data inputs: x and targets: y\n",
    "x_train_path = os.path.join(data_dir, 'X_train.hdf')\n",
    "y_train_path = os.path.join(data_dir, 'y_train.hdf')\n",
    "\n",
    "# validation data inputs: x and targest: y\n",
    "x_valid_path = os.path.join(data_dir, 'X_test.hdf')\n",
    "y_valid_path = os.path.join(data_dir, 'y_test.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01_pandas\"></a>\n",
    "## 1.2 Load the Data with <i>pandas</i>\n",
    "Finally, we load the data using the *pandas* API for reading in HDF files.  Python with pandas is used in a wide variety of academic and commercial domains, including finance, neuroscience, economics, statistics, advertising, web analytics, and more. The pandas library is an open source, BSD-licensed project providing easy-to-use data structures and analysis tools for the Python programming language. The pandas library features a fast and efficient DataFrame object for data manipulation with integrated indexing as well as tools for reading and writing data between in-memory data structures and different formats such as CSV and text files, Microsoft Excel, SQL databases, and the fast HDF5 format. Check out the [pandas documentation](http://pandas.pydata.org) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It should take less than one minute to load the files\n",
    "X_train = pd.read_hdf(x_train_path)\n",
    "y_train = pd.read_hdf(y_train_path)\n",
    "X_valid = pd.read_hdf(x_valid_path)\n",
    "y_valid = pd.read_hdf(y_valid_path)\n",
    "print('data load complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  An alternative to *pandas* is [*cuDF*](https://github.com/rapidsai/cudf/tree/master), a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating data, all in a *pandas*-like API familiar to data scientists.  For larger data sets especially, this is an alternative you may wish to try for faster processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01_explore\"></a>\n",
    "## 1.3 Visualize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This electronic health records (EHR) database contains medical treatments and histories of patients collected over time. The EHRs used here consists of 10 years worth of patient encounter data in the Pediatric Intensive Care Unit (PICU) at Children's Hospital Los Angeles, curated by the virtual PICU ([vPICU](http://vpicu.net)) team. This dataset contains 1,113,529 observations among 5,000 unique patient encounters. \n",
    "\n",
    "<img style=\"float: right;\" src=\"images/ehr.svg\" width=\"450\" height=\"300\">\n",
    "This data is an irregular time series of observations consisting of varied numbers of measurements taken over the course of a patient's stay in the PICU. Time between observations can vary from minutes to hours. A simplified diagram of the data can be seen on the right. <br>\n",
    "Features (measurement types) include:\n",
    "\n",
    "* **Statics** *(e.g. gender, age, weight)*\n",
    "* **Vitals** *(e.g. heart rate, respiratory rate)*\n",
    "* **Labs** *(e.g. glucose, creatinine)*\n",
    "* **Interventions** *(e.g. intubation, O2)*\n",
    "* **Drugs** *(e.g. dopamine, epinephrine)*\n",
    "\n",
    "For a complete list of features, see [ehr_features.csv](csv/ehr_features.csv)\n",
    "\n",
    "One thing to note is that in addition to the non-uniform sampling, not all measurement types were taken for all patients.\n",
    "\n",
    "If we just have a look at the training data, it's clear that we have a collection of patient encounters with a set of variables observed at different times during each encounter.  But again, not all variables are measured at each observation time (row entry). \n",
    "\n",
    "The label (y) data for each patient encounter is the ultimate result of alive (1) or not alive (0). \n",
    "\n",
    "Let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that there are 265 measurement variables (columns) in total.  We could also ask directly using `len(X_train.columns)`.<br>\n",
    "The data imported by is a multi-index DataFrame where index \"level 0\" is the unique patient encounter identifier (`encounterID`) and index \"level 1\" is the time of each observation in units of hours since the first observation (`absoluteTime`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a quick look at the label data for each patient encounter.  We see it follows the same indexing, but only contains a single binary value for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01_single'></a>\n",
    "### 1.3.1 Example: View Data from a Single Encounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look more closely at a single patient visit by specifying the `encounterID` \"level 0\" index.  To extract just these values, try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index.levels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _pandas_ DataFrame is easily converted to a regular Python list using `list()`, as shown in the code following code block.  To demonstrate how to manipulate DataFrame objects, we will select a random patient encounter and extract specific variables.  Run the next cell a few times to get a feel for the variety of data associated with various random patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random patient encounterID from a list of all the encounterID values\n",
    "eIdx = random.choice(list(X_train.index.levels[0]))\n",
    "\n",
    "# Specify a few variables to look at\n",
    "variables = [\n",
    "    'Age','Heart rate (bpm)','PulseOximetry','Weight',\n",
    "    'SystolicBP','DiastolicBP','Respiratory rate (bpm)',\n",
    "    'MotorResponse','Capillary refill rate (sec)'\n",
    "]\n",
    "\n",
    "# Note that the full list of variables can be constructed using\n",
    "# list(X_train.columns.values)\n",
    "\n",
    "# Have a look at the variables for the selected patient\n",
    "print('encounterID = {}'.format(eIdx))\n",
    "print('number of observations = {}'.format(X_train.loc[eIdx].index.shape[0]))\n",
    "print('max absoluteTime value = {} hours'.format(X_train.loc[eIdx].index[-1]))\n",
    "X_train.loc[eIdx, variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows tells us how many observations are included in the patient encounter, and the final `absoluteTime` indicates how many hours long the entire encounter lasted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a look at a variable for a particular patient encounter simply extract that variable from an encounter and plot it using the _pandas_ plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[eIdx, \"Heart rate (bpm)\"].plot()\n",
    "plt.ylabel(\"Heart rate (bpm)\")\n",
    "plt.xlabel(\"Hours since first encounter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01_ex_nobs\"></a>\n",
    "### 1.3.2 Exercise: Observation Count Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to know more about the variability of the time series data. For example, how many observations are there in an encounter typically?  A histogram will provide a good visualization of how the variability in observations per patient is distributed.  To create a histogram we need to:\n",
    "\n",
    "1. Create a list of `encounterID` values\n",
    "2. Create a list of observation counts for each of these encounters\n",
    "3. Create a histogram using MatplotLib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Step 1 - Create a list of `encounterID` values\n",
    "# Hint: In the single encounter example, we created this list and selected a random choice from it\n",
    "eIdx_list = [] #FIXME\n",
    "print('eIdx_list created of length {} (Sanity check: this length should be 5000)'.format(len(eIdx_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Create a list of observation counts for each of these encounters\n",
    "# The number of observations in each encounter can be extracted as the shape of each individual encounter\n",
    "nobs_list = [X_train.loc[ix].index.shape[0] for ix in eIdx_list]\n",
    "print('list of observation counts created of length {} (Sanity check: this length should also be 5000)'.format(len(nobs_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Create a histogram using MatplotLib\n",
    "plt.hist(nobs_list,range=(0,1000))\n",
    "plt.title(\"Observations per Patient Encounter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done created the initial eIdx list correctly, you should see a histogram (if not, check the [solution](#01_sol_nobs) below).  Now you can also check the mean and median counts using `np.mean()` and `np.median()` functions over the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The mean = {}'.format(np.mean(nobs_list)))\n",
    "print('The median = {}'.format(np.median(nobs_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01_ex_time'></a>\n",
    "### 1.3.3 Exercise: Length of PICU Stay Histogram\n",
    "We can do a similar analysis to determine the observation timespan over all patient encounters.  Recall that the observation index `absoluteTime` is the cumulative time since admission to the PICU.  Therefore, the last `absoluteTime` value is the one we want to capture for each encounter.  Earlier in the section, we displayed this value for an individual encounter using `X_train.loc[eIdx].index[-1]`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give it a try.  You can always check the [solution](#01_solution_1.3.2) if you wish.\n",
    "\n",
    "1. Create a list of `encounterID` values\n",
    "2. Create a list of `absoluteTime` final for each encounter\n",
    "3. Create a histogram using MatplotLib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Step 1 - Create a list of `encounterID` values\n",
    "# Hint: Earlier in the section, we created this list and selected a random choice from it\n",
    "eIdx_list = [] #FIXME\n",
    "print('eIdx_list created of length {} (Sanity check: this length should be 5000)'.format(len(eIdx_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Step 2 - Create a list of `absoluteTime` final for each encounter\n",
    "# Hint: Use a Python list comprehension over the eIdx_list\n",
    "timespan_list = [] #FIXME\n",
    "print('timespan_list created of length {} (Sanity check: this length should be 5000)'.format(len(timespan_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Create a histogram using MatplotLib\n",
    "plt.hist(timespan_list,range=(0,500))\n",
    "plt.title(\"Hours per Patient Encounter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done created the initial eIdx list correctly, you should see a histogram (if not, check the [solution](#01_sol_time) below).  Now you can also check the mean and median counts using `np.mean()` and `np.median()` functions over the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The mean = {}'.format(np.mean(timespan_list)))\n",
    "print('The median = {}'.format(np.median(timespan_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01_save\"></a>\n",
    "## 1.4 Save a *pandas* DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to the next notebook, save the *pandas* DataFrames to reload into the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame's for use in other notebooks\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "y_train.to_pickle('y_train.pkl')\n",
    "X_valid.to_pickle('X_valid.pkl')\n",
    "y_valid.to_pickle('y_valid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size=10>Great job!</font><br>\n",
    "Now that we've loaded and visualized the data, we'll prepare it to train our model.  Move on to the [next notebook](02_PrepareData.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01_solutions'></a>\n",
    "# Solutions\n",
    "<a name='01_sol_nobs'></a>\n",
    "### 1.3.2 Solution: Observation Count Histogram \n",
    "[Jump back to Exercise 1.3.2](#01_ex_nobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Step 1 - Create a list of `encounterID` values\n",
    "# Hint: In the single encounter example, we created this list and selected a random choice from it\n",
    "eIdx_list = X_train.index.levels[0] #FIXME\n",
    "print('eIdx_list created of length {} (Sanity check: this length should be 5000)'.format(len(eIdx_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Create a list of observation counts for each of these encounters\n",
    "# The number of observations in each encounter can be extracted as the shape of each individual encounter\n",
    "nobs_list = [X_train.loc[ix].index.shape[0] for ix in eIdx_list]\n",
    "print('list of observation counts created of length {} (Sanity check: this length should also be 5000)'.format(len(nobs_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Create a histogram using MatplotLib\n",
    "plt.hist(nobs_list,range=(0,1000))\n",
    "plt.title(\"Observations per Patient Encounter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01_sol_time'></a>\n",
    "### 1.3.3 Solution: Length of PICU Stay Histogram\n",
    "[Jump back to Exercise 1.3.3](#01_ex_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Step 1 - Create a list of `encounterID` values\n",
    "# Hint: Earlier in the section, we created this list and selected a random choice from it\n",
    "eIdx_list = X_train.index.levels[0] #FIXME\n",
    "print('eIdx_list created of length {} (Sanity check: this length should be 5000)'.format(len(eIdx_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Step 2 - Create a list of `absoluteTime` final for each encounter\n",
    "# Hint: Use a Python list comprehension over the eIdx_list\n",
    "timespan_list = [X_train.loc[ix].index[-1] for ix in eIdx_list] #FIXME\n",
    "print('timespan_list created of length {} (Sanity check: this length should be 5000)'.format(len(timespan_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Create a histogram using MatplotLib\n",
    "plt.hist(timespan_list,range=(0,500))\n",
    "plt.title(\"Hours per Patient Encounter\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
